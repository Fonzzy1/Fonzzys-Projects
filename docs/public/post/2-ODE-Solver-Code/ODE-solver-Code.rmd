---
title: "Taylor Series approximations of ODEs: The Code"
author: Fonzzy
date: 2021-05-18T00:00:00+00:00
output: html_document
---
```{python, echo = FALSE}
import math
import time
import numpy as np
import matplotlib.pyplot as plt
import itertools as it
import pandas as pd
import time as t


def lin_IVP(xmin, xmax, vect_initial, matrix_transform, print=True, title = ''):
    # Check for valid input information
    start = t.time()
    ## Column int Vect?
    if vect_initial.shape[1] != 1 or len(vect_initial.shape) != 2:
        print('Invalid input vector, not a column')
        exit(0)
    ## positive h?
    if vect_initial[0][0] <= 0:
        print('Requires a positive non 0 value for h')
        exit(0)
    ## Transform Matrix Correct Shape?
    if matrix_transform.shape[0] != matrix_transform.shape[1] or matrix_transform.shape[0] != vect_initial.shape[
        0] or len(matrix_transform.shape) != 2:
        print('Transformation matrix wrong shape')
        exit(0)
    ## Valid xmin and xmax?
    if xmax - xmin <= 0:
        print('xmax must be greater than xmin')
        exit(0)

    h = vect_initial[0][0]

    # Build the Stepping matrices
    matrix_step_neg = build_matrix_step(-h, matrix_transform)
    matrix_step_pos = build_matrix_step(h, matrix_transform)

    ## Create df_output Space

    df_output = pd.DataFrame(vect_initial.T)

    ## Do the positive steps first
    vect = vect_initial.copy()
    while vect[1][0] <= xmax + h:
        vect = np.matmul(matrix_step_pos, vect)
        df_output.loc[len(df_output)] = vect.T[0]

    ## Do the negative steps next
    vect = vect_initial.copy()
    while vect[1][0] > xmin:
        vect = np.matmul(matrix_step_neg, vect)
        df_output.loc[len(df_output)] = vect.T[0]

    # Reprder
    df_output.sort_values(1, inplace=True)

    end  = t.time()

    time = end - start


    if print == True:
        ## Plot df_output
        plt.plot(df_output[1], df_output[2])
        plt.plot(df_output[1], comparison(df_output[1]))
        plt.title(title + ', t = ' +str(time))
        plt.show()

    return df_output




def single_value_lin_IVP(x, vect_initial, matrix_transform):
    # Check for valid input information
    ## Column int Vect?
    if vect_initial.shape[1] != 1 or len(vect_initial.shape) != 2:
        print('Invalid input vector, not a column')
        exit(0)
    ## positive h?
    if vect_initial[0][0] <= 0:
        print('Requires a positive non 0 value for h')
        exit(0)
    ## Transform Matrix Correct Shape?
    if matrix_transform.shape[0] != matrix_transform.shape[1] or matrix_transform.shape[0] != vect_initial.shape[
        0] or len(matrix_transform.shape) != 2:
        print('Transformation matrix wrong shape')
        exit(0)

    h = vect_initial[0][0]

    # Build the Stepping matrices
    matrix_step_neg = build_matrix_step(-h, matrix_transform)
    matrix_step_pos = build_matrix_step(h, matrix_transform)

    k = int((x - vect_initial[1]) / h)

    if k > 0:
        step_matrix_k = np.linalg.matrix_float_power(matrix_step_pos, k)
        vect_output = np.matmul(step_matrix_k, vect_initial)
    if k < 0:
        step_matrix_k = np.linalg.matrix_float_power(matrix_step_neg, -k)
        vect_output = np.matmul(step_matrix_k, vect_initial)
    if k == 0:
        vect_output = vect_initial

    return vect_output


def build_matrix_step(h, matrix_transform):
    matrix_base = np.identity(len(matrix_transform))
    matrix_base[1][0] = np.sign(h)
    for i in range(2, len(matrix_base)):
        for j in range(i, len(matrix_base)):
            matrix_base[i][j] = h ** (j - i) / math.factorial(j - i)
    matrix_step = np.matmul(matrix_transform, matrix_base)
    return matrix_step

def adjustment_funtion(vector):
    vector[4] = -vector[2]
    return vector

def find_best_initial(search_min, search_max, search_step, vect_initial, matrix_transform, condition_vect):
    # Find All Possible initial vectors
    search_permutations = [
        *it.combinations_with_replacement(np.arange(search_min, search_max + search_step, search_step),
                                          np.count_nonzero(np.isnan(vect_initial)))]
    nan_location = np.argwhere(np.isnan(vect_initial))
    nan_location = nan_location[nan_location != 0]
    vect_initial_poss = np.zeros((len(search_permutations), len(vect_initial)))
    for i in range(0, len(search_permutations)):
        vect_initial_poss[i] = vect_initial.T
        vect_initial_poss[i][nan_location] = search_permutations[i]
        ## This ensures only valid input vectors are met
        vect_initial_poss[i] = np.matmul(matrix_transform,vect_initial_poss[i].T)

    ## List of scores
    ls_distance = []

    # Run over range of possible vectors
    for vect_initial in vect_initial_poss:
        ls_distance_single = []
        for vect in condition_vect:
            output = single_value_lin_IVP(vect[1], np.reshape(vect_initial, (len(vect_initial), 1)), matrix_transform)
            dist = np.linalg.norm(np.nan_to_num(output - np.reshape(vect, (len(vect), 1))))
            ls_distance_single.append(dist)
        ls_distance.append(sum(ls_distance_single))

    df_results = pd.DataFrame(vect_initial_poss)
    df_results['dist'] = ls_distance

    best_index = df_results[['dist']].idxmin()

    best_int_vect = vect_initial_poss[best_index].T
    dist = df_results['dist'][best_index].values
    return (best_int_vect, dist, df_results)

def nonlin_IVP(xmin, xmax, vect_initial, print=True):

    #Check for valid input information
    ## Column int Vect?
    if vect_initial.shape[1] != 1 or len(vect_initial.shape) != 2:
        print('Invalid input vector, not a column')
        exit(0)
    ## positive h?
    if vect_initial[0][0] <= 0:
        print('Requires a positive non 0 value for h')
        exit(0)
    ## Valid xmin and xmax?
    if xmax-xmin <= 0:
        print('xmax must be greater than xmin')
        exit(0)

    h = vect_initial[0][0]

    # Build the Stepping matrices
    matrix_step_neg = build_matrix_step_nonlin(-h, vect_initial)
    matrix_step_pos = build_matrix_step_nonlin(h, vect_initial)

    ## Create df_output Space

    df_output = pd.DataFrame(vect_initial.T)

    ## Do the positive steps first
    vect = vect_initial.copy()
    while vect[1][0] <= xmax + h:
        vect = np.matmul(matrix_step_pos, vect)
        df_output.loc[len(df_output)] = vect.T[0]

    ## Do the negative steps next
    vect = vect_initial.copy()
    while vect[1][0] > xmin:
        vect = np.matmul(matrix_step_neg, vect)
        df_output.loc[len(df_output)] = vect.T[0]

    # Reprder
    df_output.sort_values(1, inplace=True)

    if print == True:
        ## Plot df_output
        plt.plot(df_output[1], df_output[2])
        plt.title(str(vect_initial.T))
        plt.show()

    return df_output

def build_matrix_step_nonlin(h, vect_initial):
    base_matrix = np.identity(len(vect_initial))
    base_matrix[1][0] = np.sign(h)
    for i in range(2, len(base_matrix)):
        for j in range(i, len(base_matrix)):
            base_matrix[i][j] = h ** (j - i) / math.factorial(j - i)
    matix_step = base_matrix
    return matix_step

def find_best_initial_nonlin(search_min, search_max, xmin, xmax, search_step, vect_initial, condition_vect):
    # Find All Possible initial vectors
    search_permutations = [*it.combinations_with_replacement(np.arange(search_min, search_max + search_step, search_step),
                                            np.count_nonzero(np.isnan(vect_initial)))]
    nan_location = np.argwhere(np.isnan(vect_initial))
    nan_location = nan_location[nan_location != 0]
    vect_initial_poss = np.zeros((len(search_permutations), len(vect_initial)))
    for i in range(0, len(search_permutations)):
        vect_initial_poss[i] = vect_initial.T
        vect_initial_poss[i][nan_location] = search_permutations[i]
        ## This ensures only valid input vectors are met
        vect_initial_poss[i] = adjustment_funtion(vect_initial_poss[i].T)


    ## List of scores
    ls_distance = []

    # Run over range of possible vectors
    for vect_initial in vect_initial_poss:
        output = nonlin_IVP(xmin,xmax, np.reshape(vect_initial, (len(vect_initial), 1)), False)
        ls_distance_single = []
        h = vect_initial[0]
        for vect in condition_vect:
            index = int(math.floor((vect[1] - xmin) / h))
            dist = np.linalg.norm(np.nan_to_num(output[index] - vect))
            ls_distance_single.append(dist)
            ls_distance_single.append(dist)
        ls_distance.append(sum(ls_distance_single))

    df_results = pd.DataFrame(vect_initial_poss)
    df_results['dist'] = ls_distance

    best_index = df_results[['dist']].idxmin()

    best_int_vect = vect_initial_poss[best_index].T
    dist = df_results['dist'][best_index].values
    return (best_int_vect, dist, df_results)
```

Now that we have an [algorithm for approximating ODEs](https://fonzzys-projects.netlify.app/2021/05/18/taylor-series-approximations-of-odes-the-algorithm/), whe can start implementing it in the form of code so we can start using it to solve problems.

Since the code is a fairy simple implementation of the algorithm, I'm not going to go into it in much detail, but feel free to look at it on my [github](https://github.com/Fonzzy1/Fonzzys-Projects/tree/main/Differential%20Equations/One%20Dimension). What were going to look at today is how good this algorithm is and how it applies to apply it to a broad range of ODE problems.

## Well Posed ODEs
A well posed ODE meets the condition of uniqueness and existence, meaning that one and only one solution exists to the ODE. These conditions need to be met when approximating ODEs, as to ensure that the information we take from the approximation is meaningful. The well pointedness comes from the conditions placed on the ODE, which are stored within a condition vector that we used to define the system.
Since all initial vectors and stepping functions will produce a result, this problem of wellpossedness can be reduced to seeing weather or not a unique initial vector can be found that matches the conditions given the stepping function.
For an ODE of order k, the initial vector will have $k+1$ unknowns, $f(x), f'(x)...f^k(x)$, therefore $k+1$ pieces of information will be needed to uniquely determine this initial vector, and this would therefore guarantee uniqueness.
Similarly, the information about the system cannot be mutually exclusive, such as $f(0) = 2 , f(0) = 4$ as this would mean that a soulution would not exist.

The concept of well posedness is important as the algorithm will produce an output even if the problem is ill posed, therefore one will have to ensure it is well posed before applying the algorithm. This is done using this function;

```{python}
def check_well_posedness(condition_vect):
    known_conditions = []
    ## Separate each piece of info into a row
    for vect in condition_vect:
        ## See additional information defined by definition of ODE
        vect_transformed = adjustment_funtion(vect.T).T
        k = 0
        for value in vect_transformed[2:len(vect)]:
            if not np.isnan(value):
                ## [x, order of derivative, value], each row is a piece of information
                known_conditions.append([vect[1], k, value])
            k += 1

    ## Remove Duplicates
    known_conditions = np.unique(known_conditions,axis=0)

    ##Find if value is defined twice differently,
    range_of_def = []
    for i in known_conditions:
        range_of_def.append(i[0:2])

    ## Contradicting information
    if len(range_of_def) != len(np.unique(range_of_def,axis=0)):
        return False, known_conditions, 'No Solution, contradictory conditions'
    ## Returns True if correct number of elements
    elif len(known_conditions) == len(condition_vect[0]) - 2:
        return True, known_conditions,''
    ## Else will Return False with the wrong number of conditions.
    else:
        return False, known_conditions, 'Non unique solution, wrong number of few conditions'
```
## Finding valid initial vectors
For some problems, it is rather simple to determine the initial vector of the ODE, since it is given as part of the conditions. These are called initial value problems and  approximatiing the solution is rather simple as the initial vector is allready known. In contrast there are also  boundry value problems, where the condtions are placed in various places along the domain, normally at each end. The normal method to approximate these BVPs uses inverse matrices and can get quite messy, especially for large domains and small step size, however is guaranteed to give a solution on the first try. This is valuable when computational float_power is expensive or god forbid, you would have to work it out by hand, however this is not the case for us. Using the float_power of 21st century computing, we can simply brute force this problem by trying a heap of initial vectors and seeing which one best fits the condtions.

In the general case this is done using the following function;
```{python}
def find_best_initial_nonlin(search_min, search_max, xmin, xmax, search_step, vect_initial, condition_vect):
    # Find All Possible initial vectors, where unknowns are np.NAN
    search_permutations = [*it.combinations_with_replacement(np.arange(search_min, search_max + search_step, search_step),
                                            np.count_nonzero(np.isnan(vect_initial)))]
    nan_location = np.argwhere(np.isnan(vect_initial))
    nan_location = nan_location[nan_location != 0]
    vect_initial_poss = np.zeros((len(search_permutations), len(vect_initial)))
    for i in range(0, len(search_permutations)):
        vect_initial_poss[i] = vect_initial.T
        vect_initial_poss[i][nan_location] = search_permutations[i]
        ## This ensures only valid input vectors are met
        vect_initial_poss[i] = adjustment_funtion(vect_initial_poss[i].T)


    ## List of scores
    ls_distance = []

    # Run over range of possible vectors
    for vect_initial in vect_initial_poss:
        output = nonlin_IVP(xmin,xmax, np.reshape(vect_initial, (len(vect_initial), 1)), False)
        ls_distance_single = []
        h = vect_initial[0]
        for vect in condition_vect:
            index = int(math.floor((vect[1] - xmin) / h))
            dist = np.linalg.norm(np.nan_to_num(output[index] - vect))
            ls_distance_single.append(dist)
            ls_distance_single.append(dist)
        ls_distance.append(sum(ls_distance_single))

    df_results = pd.DataFrame(vect_initial_poss)
    df_results['dist'] = ls_distance

    best_index = df_results[['dist']].idxmin()

    best_int_vect = vect_initial_poss[best_index].T
    dist = df_results['dist'][best_index].values
    return (best_int_vect, dist, df_results)
```
Simulating the full range of the ODE can be quite slow, therefore a major improvement to the function can be made to the linear case.
Remembering that:
$$ \begin{bmatrix}
h\\
x+h\\
y(x+h)\\
y'(x+h)\\
y''(x+h)\\
...\\
y^{n}(x+h)\\
\end{bmatrix} = T \cdot \begin{bmatrix}
1 & 0 & 0 & 0 & 0 &  ... & 0\\
1 & 1 & 0 & 0 & 0 &  ... & 0\\
0 & 0 & 1 & \frac{h}{1!} & \frac{h^2}{2!} &  ... & \frac{h^n}{n!}\\
0 & 0 & 0 & 1 & \frac{h}{1!} &  ... & \frac{h^{n-1}}{(n-1)!}\\
0 & 0 & 0 & 0 & 1 &  ... & \frac{h^{n-2}}{(n-2)!}\\
... & ... & ... & ... & ... &  ... & ...\\
0 & 0 & 0 & 0 & 0 &  ... & 1\\
\end{bmatrix} \cdot \begin{bmatrix}
h\\
x\\
y(x)\\
y'(x)\\
y''(x)\\
...\\
y^{n}(x)\\
\end{bmatrix}$$

One can say:

$$ \begin{bmatrix}
h\\
x+ k \cdot h\\
y(x+ k \cdot h)\\
y'(x+ k \cdot h)\\
y''(x+ k \cdot h)\\
...\\
y^{n}(x+k \cdot h)\\
\end{bmatrix} = ( T \cdot \begin{bmatrix}
1 & 0 & 0 & 0 & 0 &  ... & 0\\
1 & 1 & 0 & 0 & 0 &  ... & 0\\
0 & 0 & 1 & \frac{h}{1!} & \frac{h^2}{2!} &  ... & \frac{h^n}{n!}\\
0 & 0 & 0 & 1 & \frac{h}{1!} &  ... & \frac{h^{n-1}}{(n-1)!}\\
0 & 0 & 0 & 0 & 1 &  ... & \frac{h^{n-2}}{(n-2)!}\\
... & ... & ... & ... & ... &  ... & ...\\
0 & 0 & 0 & 0 & 0 &  ... & 1\\
\end{bmatrix})^k \cdot \begin{bmatrix}
h\\
x\\
y(x)\\
y'(x)\\
y''(x)\\
...\\
y^{n}(x)\\
\end{bmatrix}$$

This property for the linear case means that we can check values without iterating over the whole domain, making the process for finding the initial vector much faster for the general case.

## Finding the approximation error
Now that we know how to apply the algorithm to a wide range of ODEs, we can start looking at how good the approximations are. Using some classic ODEs we can compare the exact solution and the approximate solution.

### Example 1

$$ f''(x) = f(x) + x $$
$$ f(0) = 0$$
$$ f'(0) = 0$$
$$ 0 \leq x \leq pi^3 $$

Converting this into the condition vector and transform matrix used int the approximations.

```{python}
condition_vect = np.array([ [np.float_power(3,0), 0, 0, 0, np.NAN]])

matrix_transform = np.array([[1, 0, 0, 0, 0],
                             [0, 1, 0, 0, 0],
                             [0, 0, 1, 0, 0],
                             [0, 0, 0, 1, 0],
                             [0, 1, 1, 0, 0]])
```
The known solution to this ODE is $f(x) = \frac{1}{2}e^{x}-\frac{1}{2}e^{-x}-x = \sinh\left(x\right)-x$, and plotting this against the approximation;
```{python, echo = FALSE}
def comparison(lst):
    return np.sinh(lst) - lst


df_output_0 = lin_IVP(0, 9, find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0], matrix_transform,title= 'h =' + str(condition_vect[0][0]))
```

```{python, echo = FALSE}
condition_vect = np.array([ [np.float_power(3,-2), 0, 0, 0, np.NAN]])
df_output_2 = lin_IVP(0, 9, find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0], matrix_transform, title= 'h =' + str(condition_vect[0][0]))
```

```{python, echo = FALSE}
condition_vect = np.array([ [np.float_power(3,-6), 0, 0, 0, np.NAN]])
df_output_6 = lin_IVP(0, 9, find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0], matrix_transform, title= 'h =' + str(condition_vect[0][0]))
```

```{python, echo = FALSE}
condition_vect =np.array([ [np.float_power(3,-1), 0, 0, 0, np.NAN]])
df_output_1 = lin_IVP(0, 9, find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0],  matrix_transform, title= 'h =' + str(condition_vect[0][0]), print=False)

condition_vect = np.array([ [np.float_power(3,-4), 0, 0, 0, np.NAN]])
df_output_4 = lin_IVP(0, 9, find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0], matrix_transform, title= 'h =' + str(condition_vect[0][0]), print=False)


condition_vect = condition_vect = np.array([ [np.float_power(3,-5), 0, 0, 0, np.NAN]])
df_output_5 = lin_IVP(0, 9, find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0], matrix_transform, title= 'h =' + str(condition_vect[0][0]), print=False)

condition_vect = condition_vect = np.array([ [np.float_power(3,-7), 0, 0, 0, np.NAN]])
df_output_7 = lin_IVP(0, 9, find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0], matrix_transform, title= 'h =' + str(condition_vect[0][0]), print=False)


condition_vect = condition_vect = np.array([ [np.float_power(3,1), 0, 0, 0, np.NAN]])
df_output__1 = lin_IVP(0, 9, find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0], matrix_transform, title= 'h =' + str(condition_vect[0][0]), print=False)

condition_vect = condition_vect = np.array([ [np.float_power(3,2), 0, 0, 0, np.NAN]])
df_output__2 = lin_IVP(0, 9, find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0], matrix_transform, title= 'h =' + str(condition_vect[0][0]), print=False)

condition_vect = condition_vect = np.array([ [np.float_power(3,-3), 0, 0, 0, np.NAN]])
df_output_3 = lin_IVP(0, 9, find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0], matrix_transform,title= 'h =' + str(condition_vect[0][0]), print=False)

condition_vect = condition_vect = np.array([ [np.float_power(3,3), 0, 0, 0, np.NAN]])
df_output__3 = lin_IVP(0, 9, find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0], matrix_transform, title= 'h =' + str(condition_vect[0][0]), print=False)
```
You can see that as we take smaller and smaller values of h, the two lines get closer and closer until they are indistinguishable. It's all well and good to say the two values are close, but we need a way to numerical determine the amount of error in the approximation. To do this, we can define an error function such that:

$$ Error(h) = \int_{x_{min}}^{x_{max}} |f_{aprox}(x) - f_{actual}(x)| \,dx  $$
$$Error(h) = \sum_{i=0}^{\frac{x_{max}-x_{min}}{h}} h \cdot |f_{aprox}(x_i) - f_{actual}(x_i)| $$

We can now plot this function for various values of h:

```{python, echo = FALSE}
values =  np.array([
    [df_output__3[0][0], sum(df_output__3[0][0]*abs(df_output__3[2] - comparison(df_output__3[1])))],
    [df_output__2[0][0], sum(df_output__2[0][0]*abs(df_output__2[2] - comparison(df_output__2[1])))],
    [df_output__1[0][0], sum(df_output__1[0][0]*abs(df_output__1[2] - comparison(df_output__1[1])))],
    [df_output_0[0][0], sum(df_output_0[0][0]*abs(df_output_0[2] - comparison(df_output_0[1])))],
    [df_output_1[0][0], sum(df_output_1[0][0]*abs(df_output_1[2] - comparison(df_output_1[1])))],
    [df_output_2[0][0], sum(df_output_2[0][0]*abs(df_output_2[2] - comparison(df_output_2[1])))],
    [df_output_3[0][0], sum(df_output_3[0][0]*abs(df_output_3[2] - comparison(df_output_3[1])))],
    [df_output_4[0][0], sum(df_output_4[0][0]*abs(df_output_4[2] - comparison(df_output_4[1])))],
    [df_output_5[0][0], sum(df_output_5[0][0]*abs(df_output_5[2] - comparison(df_output_5[1])))],
    [df_output_6[0][0], sum(df_output_6[0][0]*abs(df_output_6[2] - comparison(df_output_6[1])))],
    [df_output_7[0][0], sum(df_output_7[0][0]*abs(df_output_7[2] - comparison(df_output_7[1])))]
])

plt.plot(np.log(values.T[0]),np.log(values.T[1]))
plt.plot(np.arange(-8,4), np.arange(-8,4)*1,  ls = 'dashed')

plt.title('ln(h) vs ln(Error(h))')
plt.legend(['Error Function', 'Slope 1'])
```

Looking at this line, we can say that the function, $Error(h) = O(h)$ however this seems inconsistent with the $O(h^2)$ error found in the previous part. However, we need to remember that the two errors are different

```{python, echo = FALSE}
def comparison(lst):
    return np.sinh(lst) - lst


df_output_0 = lin_IVP(0, find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0][0], find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0], matrix_transform,title= 'h =' + str(condition_vect[0][0]),print=False)
```

```{python, echo = FALSE}
condition_vect = np.array([ [np.float_power(3,-2), 0, 0, 0, np.NAN]])
df_output_2 = lin_IVP(0, find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0][0], find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0], matrix_transform, title= 'h =' + str(condition_vect[0][0]),print=False)
```

```{python, echo = FALSE}
condition_vect = np.array([ [np.float_power(3,-6), 0, 0, 0, np.NAN]])
df_output_6 = lin_IVP(0, find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0][0], find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0], matrix_transform, title= 'h =' + str(condition_vect[0][0]),print=False)
```

```{python, echo = FALSE}
condition_vect =np.array([ [np.float_power(3,-1), 0, 0, 0, np.NAN]])
df_output_1 = lin_IVP(0, find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0][0], find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0],  matrix_transform, title= 'h =' + str(condition_vect[0][0]), print=False)

condition_vect = np.array([ [np.float_power(3,-4), 0, 0, 0, np.NAN]])
df_output_4 = lin_IVP(0, find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0][0], find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0], matrix_transform, title= 'h =' + str(condition_vect[0][0]), print=False)


condition_vect = condition_vect = np.array([ [np.float_power(3,-5), 0, 0, 0, np.NAN]])
df_output_5 = lin_IVP(0, find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0][0], find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0], matrix_transform, title= 'h =' + str(condition_vect[0][0]), print=False)

condition_vect = condition_vect = np.array([ [np.float_power(3,-7), 0, 0, 0, np.NAN]])
df_output_7 = lin_IVP(0, find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0][0], find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0], matrix_transform, title= 'h =' + str(condition_vect[0][0]), print=False)




condition_vect = condition_vect = np.array([ [np.float_power(3,-3), 0, 0, 0, np.NAN]])
df_output_3 = lin_IVP(0, find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0][0], find_best_initial(-0.2, 0, 0.001, condition_vect[0].T, matrix_transform, condition_vect)[0], matrix_transform,title= 'h =' + str(condition_vect[0][0]), print=False)

```


```{python, echo = FALSE}
values =  np.array([
    [df_output_2[0][0], sum(df_output_2[0][0]*abs(df_output_2[2] - comparison(df_output_2[1])))],
    [df_output_3[0][0], sum(df_output_3[0][0]*abs(df_output_3[2] - comparison(df_output_3[1])))],
    [df_output_4[0][0], sum(df_output_4[0][0]*abs(df_output_4[2] - comparison(df_output_4[1])))],
    [df_output_5[0][0], sum(df_output_5[0][0]*abs(df_output_5[2] - comparison(df_output_5[1])))],
    [df_output_6[0][0], sum(df_output_6[0][0]*abs(df_output_6[2] - comparison(df_output_6[1])))],
    [df_output_7[0][0], sum(df_output_7[0][0]*abs(df_output_7[2] - comparison(df_output_7[1])))]
])

plt.plot(np.log(values.T[0]),np.log(values.T[1]))
plt.plot(np.arange(-8,-2), np.arange(-8,-2)*3,  ls = 'dashed')

plt.title('ln(h) vs Ln(error for one step)')
plt.legend(['Error Function', 'Slope 3'])
```


