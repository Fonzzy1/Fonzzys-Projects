---
title: "Taylor Series for approximating ODEs"
author: Fonzzy
date: 2021-04-26T00:00:00+00:00
output: html_document
---



<p>If you ever start doing university level maths, physics, chemistry or even economics, you will most likely start running into differential equations. These are equations that describe functions in terms of their derivatives. Normally in these classes, at least at the start, all the problems are solvable by hand with some fairly rudimentary algebra and calculus. However, if you ever try and work with these problems in the real world, you will quickly realise that a lot of differential equations aren’t solvable.</p>
<p>This is where numerical methods become useful as they provide one the ability to approximate the solution of the differential equation without explicitly soliving it.</p>
<div id="eulers-method" class="section level2">
<h2>Euler’s Method</h2>
<p>One of the first numerical methods that most people get taught is Euler’s method. There are many <a href="https://tutorial.math.lamar.edu/classes/de/eulersmethod.aspx">better writers</a> who can explain this method much better than me, so I won’t go into too much detail. Essentially, this method uses the definition of the differential equation to calculate steps of a given length h. For an ODE (differential equation of one variable) of order n, the method can be described as:</p>
<p><span class="math display">\[ \begin{bmatrix}
y(x+h)\\
y&#39;(x+h)\\
y&#39;&#39;(x+h)\\
...\\
y^{n-1}(x+h)\\
\end{bmatrix} = \begin{bmatrix}
y(x)\\
y&#39;(x)\\
y&#39;&#39;(x)\\
...\\
y^{n-1}(x)\\
\end{bmatrix} + \begin{bmatrix}
h &amp; 0 &amp; 0 &amp;  ... &amp; 0\\
0 &amp; h &amp; 0 &amp;  ... &amp; 0\\
0 &amp; 0 &amp; h &amp;  ... &amp; 0\\
... &amp; ... &amp; ... &amp;  ... &amp; ...\\
0 &amp; 0 &amp; 0 &amp;  ... &amp; h\\
\end{bmatrix} \cdot \begin{bmatrix}
y&#39;(x)\\
y&#39;&#39;(x)\\
y&#39;&#39;&#39;(x)\\
...\\
y^{n}(x)\\
\end{bmatrix}\]</span></p>
<p>This can also be written as:</p>
<p><span class="math display">\[ \begin{bmatrix}
y(x+h)\\
y&#39;(x+h)\\
y&#39;&#39;(x+h)\\
...\\
y^{n}(x+h)\\
\end{bmatrix} = \begin{bmatrix}
1 &amp; h &amp; 0 &amp;  ... &amp; 0\\
0 &amp; 1 &amp; h &amp;  ... &amp; 0\\
0 &amp; 0 &amp; 1 &amp;  ... &amp; 0\\
... &amp; ... &amp; ... &amp;  ... &amp; ...\\
0 &amp; 0 &amp; 0 &amp;  ... &amp; 1\\
\end{bmatrix} \cdot \begin{bmatrix}
y(x)\\
y&#39;(x)\\
y&#39;&#39;(x)\\
...\\
y^{n}(x)\\
\end{bmatrix}\]</span></p>
<div id="how-to-expand-the-euler-method" class="section level3">
<h3>How to expand the Euler method?</h3>
<p>The Euler Method approximates functions as straight lines at each point and then approximates the function again at the next point. However, if the function is not much like a straight line then this approximation will fail to produce accurate results. Therefore, the question becomes how could one take into account the curve of the function between steps.</p>
</div>
</div>
<div id="taylor-series" class="section level2">
<h2>Taylor Series</h2>
<p>A solution to the problem posed above is to use a taylor series of the function, which describes a function as a polynomial determined by its derivatives. Explicitly, it is defined as :</p>
<p><span class="math display">\[ y(x) =  \sum_{n = 0}^{\infty}  \frac{\mathbb y^{n}(a)}{n!}\cdot(x - a)^n \]</span></p>
<p>Where <span class="math inline">\(a\)</span> is the point where the approximation is built.</p>
<p>When looking at the point <span class="math inline">\(a + h\)</span> exclusively, this formula transforms to:</p>
<p><span class="math display">\[ y(a+h) = \sum_{n = 0}^{\infty}  \frac{\mathbb y^{n}(a)}{n!}\cdot(h)^n \]</span></p>
<p>A nice property of the taylor series is that it has a really simple derivative function:</p>
<p><span class="math display">\[ y&#39;(a+h) = \sum_{n = 0}^{\infty}  \frac{\mathbb y^{n+1}(a)}{n!}\cdot(h)^n \]</span>
<span class="math display">\[ y^m(a+h) = \sum_{n = 0}^{\infty}  \frac{\mathbb y^{n+m}(a)}{n!}\cdot(h)^n \]</span></p>
<p>For a given h, this function describes a linear combination of the derivatives of a function, which can then be writen as a matrix multiplication</p>
<p><span class="math display">\[ \begin{bmatrix}
y(x+h)\\
y&#39;(x+h)\\
y&#39;&#39;(x+h)\\
...\\
y^{n}(x+h)\\
\end{bmatrix} = \begin{bmatrix}
1 &amp; \frac{h}{1!} &amp; \frac{h^2}{2!} &amp;  ... &amp; \frac{h^n}{n!}\\
0 &amp; 1 &amp; \frac{h}{1!} &amp;  ... &amp; \frac{h^{n-1}}{(n-1)!}\\
0 &amp; 0 &amp; 1 &amp;  ... &amp; \frac{h^{n-2}}{(n-2)!}\\
... &amp; ... &amp; ... &amp;  ... &amp; ...\\
0 &amp; 0 &amp; 0 &amp;  ... &amp; 1\\
\end{bmatrix} \cdot \begin{bmatrix}
y&#39;(x)\\
y&#39;&#39;(x)\\
y&#39;&#39;&#39;(x)\\
...\\
y^{n}(x)\\
\end{bmatrix}\]</span></p>
<p>One can see that this step matrix is similar to the matrix that defines the steps of the Euler method, however the Euler method doesn’t take into account each of the higher derivatives, whereas this new step matrix does.</p>
<p>Now expanding the input vector to contain x and h information;</p>
<p><span class="math display">\[ \begin{bmatrix}
h\\
x+h\\
y(x+h)\\
y&#39;(x+h)\\
y&#39;&#39;(x+h)\\
...\\
y^{n}(x+h)\\
\end{bmatrix} = \begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp;  ... &amp; 0\\
1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp;  ... &amp; 0\\
0 &amp; 0 &amp; 1 &amp; \frac{h}{1!} &amp; \frac{h^2}{2!} &amp;  ... &amp; \frac{h^n}{n!}\\
0 &amp; 0 &amp; 0 &amp; 1 &amp; \frac{h}{1!} &amp;  ... &amp; \frac{h^{n-1}}{(n-1)!}\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp;  ... &amp; \frac{h^{n-2}}{(n-2)!}\\
... &amp; ... &amp; ... &amp; ... &amp; ... &amp;  ... &amp; ...\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp;  ... &amp; 1\\
\end{bmatrix} \cdot \begin{bmatrix}
h\\
x\\
y(x)\\
y&#39;(x)\\
y&#39;&#39;(x)\\
...\\
y^{n}(x)\\
\end{bmatrix}\]</span></p>
<p>For the sake of simplicity, this equation can referred to as <span class="math inline">\(Y(x+h) = S \cdot Y(x)\)</span></p>
</div>
<div id="how-to-make-the-approximation-fit-the-ode" class="section level2">
<h2>How to make the approximation fit the ODE</h2>
<p>This pert of the problem has to be turned into two separate problems, one for ODEs that are exclusively linear combinations of the input vector and one they are not.</p>
<div id="case-1-exclusively-linear" class="section level4">
<h4>Case 1 : Exclusively linear</h4>
<p>For an ODE that is a linear combination of x, y and y’s derivatives, such as <span class="math inline">\(y&#39;&#39;=x+y\)</span>, on can simply change the stepping matrix to reflect this. This can be done by applying a transformation matrix <span class="math inline">\(T\)</span> to the stepping matrix. Knowing that each row of the stepping matrix represents one of h, x, y or y’s derivatives, creating a transformation matrix is fairly easy.</p>
<p>Sticking with the same sample ODE <span class="math inline">\(y&#39;&#39;=x+y\)</span>, the transformation matrix can be defined as;</p>
<p><span class="math display">\[
T_1 = \begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\
0 &amp; 1 &amp; 1 &amp; 0 &amp; 0\\
\end{bmatrix}
\]</span></p>
<p>However, thinking of this ODE as <span class="math inline">\(x = y&#39;&#39; - y\)</span> and <span class="math inline">\(y = y&#39;&#39; - x\)</span> is is just as reasonable to define T as:</p>
<p><span class="math display">\[
T_2 = \begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; -1 &amp; 0 &amp; 1\\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\
\end{bmatrix}
\]</span></p>
<p>or</p>
<p><span class="math display">\[
T_3 = \begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; -1 &amp; 0 &amp; 0 &amp; 1\\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\
\end{bmatrix}
\]</span></p>
<p>So what is best option for the approximation. It’s not yet obvious, so lets look at the resulting step matrices from these transformations <span class="math inline">\(S^*\)</span></p>
<p><span class="math display">\[
S^*_1 = \begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
1 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1 &amp; h &amp; \frac{h^2}{2}\\
0 &amp; 0 &amp; 0 &amp; 1 &amp; h\\
1 &amp; 1 &amp; 1 &amp; h &amp; \frac{h^2}{2}\\
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
S^*_2 = \begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp;  -1 &amp; -h &amp; 1-\frac{h^2}{2}\\
0 &amp; 0 &amp; 1 &amp; h &amp; \frac{h^2}{2}\\
0 &amp; 0 &amp; 0 &amp; 1 &amp; h\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
S^*_3 = \begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
1 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\
-1 &amp; -1 &amp; 0 &amp; 0 &amp; 1\\
0 &amp; 0 &amp; 0 &amp; 1 &amp; h\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\
\end{bmatrix}
\]</span></p>
<p>Now we have to look at an important part of approximations, truncation error, that is, as h gets smaller, how quickly does the approximation converge to the true function. Since this approximation is based on the taylor series, the truncation error is the remaining terms in the approximation. For an aproximation with the greatest term k, the truncation error is therefore:</p>
<p><span class="math display">\[ r = \sum_{n = k+1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\]</span></p>
<p>Since 5 curves are being simulated at the same time, this remainder still be seen as a vector R.</p>
<p>For the standard stepping matrix for the order two ODE:</p>
<p><span class="math display">\[\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
1 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1 &amp; h &amp; \frac{h^2}{2}\\
0 &amp; 0 &amp; 0 &amp; 1 &amp; h\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\
\end{bmatrix} \rightarrow R =  \begin{bmatrix}
0\\
0\\
\sum_{n = 3}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 2}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\end{bmatrix}
\]</span></p>
<p>More generally for an ODE of order k;</p>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp;  ... &amp; 0\\
1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp;  ... &amp; 0\\
0 &amp; 0 &amp; 1 &amp; \frac{h}{1!} &amp; \frac{h^2}{2!} &amp;  ... &amp; \frac{h^k}{k!}\\
0 &amp; 0 &amp; 0 &amp; 1 &amp; \frac{h}{1!} &amp;  ... &amp; \frac{h^{k-1}}{(k-1)!}\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp;  ... &amp; \frac{h^{k-2}}{(k-2)!}\\
... &amp; ... &amp; ... &amp; ... &amp; ... &amp;  ... &amp; ...\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp;  ... &amp; 1\\
\end{bmatrix}
\rightarrow R =  \begin{bmatrix}
0\\
0\\
\sum_{n = k+1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = k}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = k-1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
...\\
\sum_{n = 1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\end{bmatrix}
\]</span></p>
<p>Returning to the example ODE, applying the relevant transformation matrices to <span class="math inline">\(R\)</span> will provide the truncation error for each <span class="math inline">\(T\)</span>:</p>
<p><span class="math display">\[R_1 = \begin{bmatrix}
0\\
0\\
\sum_{n = 3}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 2}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
 0 + \sum_{n = 3}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\end{bmatrix}
=
\begin{bmatrix}
0\\
0\\
\sum_{n = 3}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 2}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 3}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\end{bmatrix}\]</span></p>
<p><span class="math display">\[R_2 = \begin{bmatrix}
0\\
\sum_{n = 1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n-\sum_{n = 3}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 3}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 2}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\end{bmatrix}
=
\begin{bmatrix}
0\\
\mathbb y&#39;(x)\cdot(h) + \frac{\mathbb y&#39;&#39;(x)}{2!}\cdot(h)^2\\
\sum_{n = 3}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 2}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\end{bmatrix}\]</span></p>
<p><span class="math display">\[R_3 = \begin{bmatrix}
0\\
0\\
0-\sum_{n = 1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 2}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\end{bmatrix}
=
\begin{bmatrix}
0\\
0\\
\sum_{n = 1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 2}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\end{bmatrix}\]</span></p>
<p>Now we have these remainder vectors, we can look at what transformations provide the minimal truncation error, and therefore the best approximation of the ODE.</p>
</div>
</div>
